{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Spark session...\n",
      "Spark master: yarn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 13:20:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import itertools\n",
    "from itertools import islice\n",
    "from itertools import product\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window, DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, row_number, expr,explode\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType, StringType,StructType, StructField\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from google.cloud import storage\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml.feature import StringIndexerModel\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "from pyspark.ml.feature import IndexToString\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Tạo session mới\n",
    "print(\"Creating new Spark session...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDb\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.memory\", \"15g\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://192.168.0.103:27017/shopDev.Shops\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://192.168.0.103:27017/shopDev.Shops\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark master:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.executor.instances: 3\n",
      "spark.executor.cores: 4\n",
      "spark.executor.memory: 25g\n",
      "spark.driver.memory: 25g\n",
      "spark.sql.shuffle.partitions: 100\n",
      "spark.kryoserializer.buffer.max: 1024m\n",
      "spark.kryoserializer.buffer: 64m\n"
     ]
    }
   ],
   "source": [
    "# spark.stop()\n",
    "conf = dict(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "print(\"spark.executor.instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"spark.executor.cores:\", conf.get(\"spark.executor.cores\"))\n",
    "print(\"spark.executor.memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"spark.driver.memory:\", conf.get(\"spark.driver.memory\"))\n",
    "print(\"spark.sql.shuffle.partitions:\", conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(\"spark.kryoserializer.buffer.max:\", conf.get(\"spark.kryoserializer.buffer.max\"))\n",
    "print(\"spark.kryoserializer.buffer:\", conf.get(\"spark.kryoserializer.buffer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = \"team-15\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ALSModel.load(f\"gs://{bucket_name}/als_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = model.itemFactors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_factors = model.itemFactors.withColumnRenamed(\"id\", \"item_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cast sang int để khớp với itemFactors\n",
    "df_items = df_spark.withColumn(\"item_index\", col(\"item_index\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "# Hàm tính cosine similarity\n",
    "def cosine_sim(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "cosine_sim_udf = udf(cosine_sim, FloatType())\n",
    "\n",
    "\n",
    "target_items = item_factors.join(df_items, on=\"item_index\", how=\"inner\")\n",
    "\n",
    "\n",
    "item_pairs = target_items.alias(\"a\").crossJoin(item_factors.alias(\"b\")) \\\n",
    "    .filter(col(\"a.item_index\") != col(\"b.item_index\")) \\\n",
    "    .withColumn(\"similarity\", cosine_sim_udf(col(\"a.features\"), col(\"b.features\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"a.item_index\").orderBy(col(\"similarity\").desc())\n",
    "\n",
    "top_similar_items = item_pairs \\\n",
    "    .withColumn(\"rank\", row_number().over(windowSpec)) \\\n",
    "    .filter(col(\"rank\") <= 5) \\\n",
    "    .select(\n",
    "        col(\"a.item_index\").alias(\"item\"),\n",
    "        col(\"b.item_index\").alias(\"similar_item\"),\n",
    "        \"similarity\"\n",
    "    )\n",
    "\n",
    "top_similar_items.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_similar_items.write.mode(\"overwrite\").csv(f\"gs://{bucket_name}/item_based\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
