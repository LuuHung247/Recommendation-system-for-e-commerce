{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Spark session...\n",
      "Spark master: yarn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 14:17:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import itertools\n",
    "from itertools import islice\n",
    "from itertools import product\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window, DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, row_number, expr\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType, IntegerType, LongType, StringType,StructType, StructField\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from google.cloud import storage\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml.feature import StringIndexerModel\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.mllib.evaluation import RegressionMetrics, RankingMetrics\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Tạo session mới\n",
    "print(\"Creating new Spark session...\")\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"ALS\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark master:\", spark.sparkContext.master)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.executor.instances: 2\n",
      "spark.executor.cores: 4\n",
      "spark.executor.memory: 25g\n",
      "spark.driver.memory: 25g\n",
      "spark.sql.shuffle.partitions: 100\n",
      "spark.kryoserializer.buffer.max: 1024m\n",
      "spark.kryoserializer.buffer: 64m\n"
     ]
    }
   ],
   "source": [
    "# spark.stop()\n",
    "conf = dict(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "print(\"spark.executor.instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"spark.executor.cores:\", conf.get(\"spark.executor.cores\"))\n",
    "print(\"spark.executor.memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"spark.driver.memory:\", conf.get(\"spark.driver.memory\"))\n",
    "print(\"spark.sql.shuffle.partitions:\", conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(\"spark.kryoserializer.buffer.max:\", conf.get(\"spark.kryoserializer.buffer.max\"))\n",
    "print(\"spark.kryoserializer.buffer:\", conf.get(\"spark.kryoserializer.buffer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default column names\n",
    "DEFAULT_USER_COL = \"userID\"\n",
    "DEFAULT_ITEM_COL = \"itemID\"\n",
    "DEFAULT_RATING_COL = \"rating\"\n",
    "DEFAULT_LABEL_COL = \"label\"\n",
    "DEFAULT_TITLE_COL = \"title\"\n",
    "DEFAULT_GENRE_COL = \"genre\"\n",
    "DEFAULT_RELEVANCE_COL = \"relevance\"\n",
    "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
    "DEFAULT_PREDICTION_COL = \"prediction\"\n",
    "DEFAULT_SIMILARITY_COL = \"sim\"\n",
    "DEFAULT_ITEM_FEATURES_COL = \"features\"\n",
    "DEFAULT_ITEM_SIM_MEASURE = \"item_cooccurrence_count\"\n",
    "\n",
    "\n",
    "COL_DICT = {\n",
    "    \"col_user\": DEFAULT_USER_COL,\n",
    "    \"col_item\": DEFAULT_ITEM_COL,\n",
    "    \"col_rating\": DEFAULT_RATING_COL,\n",
    "    \"col_prediction\": DEFAULT_PREDICTION_COL,\n",
    "}\n",
    "\n",
    "# Filtering variables\n",
    "DEFAULT_K = 10\n",
    "DEFAULT_THRESHOLD = 10\n",
    "\n",
    "# Other\n",
    "SEED = 42\n",
    "\n",
    "client = storage.Client()\n",
    "bucket_name = \"team15-storage\"\n",
    "bucket = client.bucket(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_split_ratio(ratio):\n",
    "    \"\"\"Generate split ratio lists.\n",
    "\n",
    "    Args:\n",
    "        ratio (float or list): a float number that indicates split ratio or a list of float\n",
    "        numbers that indicate split ratios (if it is a multi-split).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "        - bool: A boolean variable multi that indicates if the splitting is multi or single.\n",
    "        - list: A list of normalized split ratios.\n",
    "    \"\"\"\n",
    "    if isinstance(ratio, float):\n",
    "        if ratio <= 0 or ratio >= 1:\n",
    "            raise ValueError(\"Split ratio has to be between 0 and 1\")\n",
    "\n",
    "        multi = False\n",
    "    elif isinstance(ratio, list):\n",
    "        if any([x <= 0 for x in ratio]):\n",
    "            raise ValueError(\n",
    "                \"All split ratios in the ratio list should be larger than 0.\"\n",
    "            )\n",
    "\n",
    "        # normalize split ratios if they are not summed to 1\n",
    "        if math.fsum(ratio) != 1.0:\n",
    "            ratio = [x / math.fsum(ratio) for x in ratio]\n",
    "\n",
    "        multi = True\n",
    "    else:\n",
    "        raise TypeError(\"Split ratio should be either float or a list of floats.\")\n",
    "\n",
    "    return multi, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_random_split(data, ratio=0.75, seed=42):\n",
    "    \"\"\"Spark random splitter.\n",
    "\n",
    "    Randomly split the data into several splits.\n",
    "\n",
    "    Args:\n",
    "        data (pyspark.sql.DataFrame): Spark DataFrame to be split.\n",
    "        ratio (float or list): Ratio for splitting data. If it is a single float number\n",
    "            it splits data into two halves and the ratio argument indicates the ratio of\n",
    "            training data set; if it is a list of float numbers, the splitter splits\n",
    "            data into several portions corresponding to the split ratios. If a list\n",
    "            is provided and the ratios are not summed to 1, they will be normalized.\n",
    "        seed (int): Seed.\n",
    "\n",
    "    Returns:\n",
    "        list: Splits of the input data as pyspark.sql.DataFrame.\n",
    "    \"\"\"\n",
    "    multi_split, ratio = process_split_ratio(ratio)\n",
    "\n",
    "    if multi_split:\n",
    "        return data.randomSplit(ratio, seed=seed)\n",
    "    else:\n",
    "        return data.randomSplit([ratio, 1 - ratio], seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkRatingEvaluation:\n",
    "    \"\"\"Spark Rating Evaluator\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rating_true,\n",
    "        rating_pred,\n",
    "        col_user=DEFAULT_USER_COL,\n",
    "        col_item=DEFAULT_ITEM_COL,\n",
    "        col_rating=DEFAULT_RATING_COL,\n",
    "        col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    ):\n",
    "        \"\"\"Initializer.\n",
    "\n",
    "        Args:\n",
    "            rating_true (pyspark.sql.DataFrame): True labels.\n",
    "            rating_pred (pyspark.sql.DataFrame): Predicted labels.\n",
    "            col_user (str): column name for user.\n",
    "            col_item (str): column name for item.\n",
    "            col_rating (str): column name for rating.\n",
    "            col_prediction (str): column name for prediction.\n",
    "        \"\"\"\n",
    "        self.rating_true = rating_true\n",
    "        self.rating_pred = rating_pred\n",
    "        self.col_user = col_user\n",
    "        self.col_item = col_item\n",
    "        self.col_rating = col_rating\n",
    "        self.col_prediction = col_prediction\n",
    "\n",
    "        # Check if inputs are Spark DataFrames.\n",
    "        self._validate_dataframes()\n",
    "\n",
    "        # Repartitioning for performance optimization.\n",
    "        self.rating_true = self.rating_true.repartition(200, col(self.col_user))\n",
    "        self.rating_pred = self.rating_pred.repartition(200, col(self.col_user))\n",
    "\n",
    "        # Select necessary columns and cast types for consistency\n",
    "        self.rating_true = self.rating_true.select(\n",
    "            col(self.col_user),\n",
    "            col(self.col_item),\n",
    "            col(self.col_rating).cast(\"double\").alias(\"label\"),\n",
    "        )\n",
    "        self.rating_pred = self.rating_pred.select(\n",
    "            col(self.col_user),\n",
    "            col(self.col_item),\n",
    "            col(self.col_prediction).cast(\"double\").alias(\"prediction\"),\n",
    "        )\n",
    "\n",
    "        # Perform the join operation\n",
    "        self.y_pred_true = self.rating_true.join(\n",
    "            self.rating_pred, [self.col_user, self.col_item], \"inner\"\n",
    "        ).drop(self.col_user).drop(self.col_item)\n",
    "\n",
    "        # Persist the joined data for repeated access.\n",
    "        self.y_pred_true.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "        # Initialize metrics calculation.\n",
    "        self.metrics = RegressionMetrics(self.y_pred_true.rdd.map(lambda x: (x.prediction, x.label)))\n",
    "\n",
    "    def _validate_dataframes(self):\n",
    "        \"\"\"Helper function to validate input DataFrames.\"\"\"\n",
    "        if not isinstance(self.rating_true, DataFrame):\n",
    "            raise TypeError(\"rating_true should be a Spark DataFrame.\")\n",
    "        if not isinstance(self.rating_pred, DataFrame):\n",
    "            raise TypeError(\"rating_pred should be a Spark DataFrame.\")\n",
    "\n",
    "        true_columns = self.rating_true.columns\n",
    "        pred_columns = self.rating_pred.columns\n",
    "\n",
    "        if self.col_user not in true_columns or self.col_item not in true_columns or self.col_rating not in true_columns:\n",
    "            raise ValueError(\"Schema of rating_true is missing one or more required columns.\")\n",
    "        if self.col_user not in pred_columns or self.col_item not in pred_columns or self.col_prediction not in pred_columns:\n",
    "            raise ValueError(\"Schema of rating_pred is missing one or more required columns.\")\n",
    "\n",
    "        if self.rating_true.count() == 0 or self.rating_pred.count() == 0:\n",
    "            raise ValueError(\"Empty input DataFrame.\")\n",
    "\n",
    "    def rmse(self):\n",
    "        \"\"\"Calculate Root Mean Squared Error.\"\"\"\n",
    "        return self.metrics.rootMeanSquaredError\n",
    "\n",
    "    def mae(self):\n",
    "        \"\"\"Calculate Mean Absolute Error.\"\"\"\n",
    "        return self.metrics.meanAbsoluteError\n",
    "\n",
    "    def rsquared(self):\n",
    "        \"\"\"Calculate R squared.\"\"\"\n",
    "        return self.metrics.r2\n",
    "\n",
    "    def exp_var(self):\n",
    "        \"\"\"Calculate explained variance.\"\"\"\n",
    "        # Use var() calculation directly on the DataFrame\n",
    "        variance_diff = self.y_pred_true.selectExpr(\"variance(label - prediction)\").collect()[0][0]\n",
    "        variance_label = self.y_pred_true.selectExpr(\"variance(label)\").collect()[0][0]\n",
    "\n",
    "        if variance_diff is None or variance_label is None:\n",
    "            return -np.inf\n",
    "        else:\n",
    "            return 1 - (variance_diff / variance_label) if variance_label != 0 else -np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkRankingEvaluation:\n",
    "    \"\"\"Spark Ranking Evaluator\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rating_true,\n",
    "        rating_pred,\n",
    "        k=DEFAULT_K,\n",
    "        relevancy_method=\"top_k\",\n",
    "        col_user=DEFAULT_USER_COL,\n",
    "        col_item=DEFAULT_ITEM_COL,\n",
    "        col_rating=DEFAULT_RATING_COL,\n",
    "        col_prediction=DEFAULT_PREDICTION_COL,\n",
    "        threshold=DEFAULT_THRESHOLD,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        This is the Spark version of ranking metrics evaluator.\n",
    "        The methods of this class, calculate ranking metrics such as precision@k, recall@k, ndcg@k, and mean average\n",
    "        precision.\n",
    "\n",
    "        The implementations of precision@k, ndcg@k, and mean average precision are referenced from Spark MLlib, which\n",
    "        can be found at `the link <https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html#ranking-systems>`_.\n",
    "\n",
    "        Args:\n",
    "            rating_true (pyspark.sql.DataFrame): DataFrame of true rating data (in the\n",
    "                format of customerID-itemID-rating tuple).\n",
    "            rating_pred (pyspark.sql.DataFrame): DataFrame of predicted rating data (in\n",
    "                the format of customerID-itemID-rating tuple).\n",
    "            col_user (str): column name for user.\n",
    "            col_item (str): column name for item.\n",
    "            col_rating (str): column name for rating.\n",
    "            col_prediction (str): column name for prediction.\n",
    "            k (int): number of items to recommend to each user.\n",
    "            relevancy_method (str): method for determining relevant items. Possible\n",
    "                values are \"top_k\", \"by_time_stamp\", and \"by_threshold\".\n",
    "            threshold (float): threshold for determining the relevant recommended items.\n",
    "                This is used for the case that predicted ratings follow a known\n",
    "                distribution. NOTE: this option is only activated if `relevancy_method` is\n",
    "                set to \"by_threshold\".\n",
    "        \"\"\"\n",
    "        self.rating_true = rating_true\n",
    "        self.rating_pred = rating_pred\n",
    "        self.col_user = col_user\n",
    "        self.col_item = col_item\n",
    "        self.col_rating = col_rating\n",
    "        self.col_prediction = col_prediction\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Check if inputs are Spark DataFrames.\n",
    "        if not isinstance(self.rating_true, DataFrame):\n",
    "            raise TypeError(\n",
    "                \"rating_true should be but is not a Spark DataFrame\"\n",
    "            )  # pragma : No Cover\n",
    "\n",
    "        if not isinstance(self.rating_pred, DataFrame):\n",
    "            raise TypeError(\n",
    "                \"rating_pred should be but is not a Spark DataFrame\"\n",
    "            )  # pragma : No Cover\n",
    "\n",
    "        # Check if columns exist.\n",
    "        true_columns = self.rating_true.columns\n",
    "        pred_columns = self.rating_pred.columns\n",
    "\n",
    "        if self.col_user not in true_columns:\n",
    "            raise ValueError(\n",
    "                \"Schema of rating_true not valid. Missing User Col: \"\n",
    "                + str(true_columns)\n",
    "            )\n",
    "        if self.col_item not in true_columns:\n",
    "            raise ValueError(\"Schema of rating_true not valid. Missing Item Col\")\n",
    "        if self.col_rating not in true_columns:\n",
    "            raise ValueError(\"Schema of rating_true not valid. Missing Rating Col\")\n",
    "\n",
    "        if self.col_user not in pred_columns:\n",
    "            raise ValueError(\n",
    "                \"Schema of rating_pred not valid. Missing User Col\"\n",
    "            )  # pragma : No Cover\n",
    "        if self.col_item not in pred_columns:\n",
    "            raise ValueError(\n",
    "                \"Schema of rating_pred not valid. Missing Item Col\"\n",
    "            )  # pragma : No Cover\n",
    "        if self.col_prediction not in pred_columns:\n",
    "            raise ValueError(\"Schema of rating_pred not valid. Missing Prediction Col\")\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "        relevant_func = {\n",
    "            \"top_k\": _get_top_k_items,\n",
    "            \"by_time_stamp\": _get_relevant_items_by_timestamp,\n",
    "            \"by_threshold\": _get_relevant_items_by_threshold,\n",
    "        }\n",
    "\n",
    "        if relevancy_method not in relevant_func:\n",
    "            raise ValueError(\n",
    "                \"relevancy_method should be one of {}\".format(\n",
    "                    list(relevant_func.keys())\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.rating_pred = (\n",
    "            relevant_func[relevancy_method](\n",
    "                dataframe=self.rating_pred,\n",
    "                col_user=self.col_user,\n",
    "                col_item=self.col_item,\n",
    "                col_rating=self.col_prediction,\n",
    "                threshold=self.threshold,\n",
    "            )\n",
    "            if relevancy_method == \"by_threshold\"\n",
    "            else relevant_func[relevancy_method](\n",
    "                dataframe=self.rating_pred,\n",
    "                col_user=self.col_user,\n",
    "                col_item=self.col_item,\n",
    "                col_rating=self.col_prediction,\n",
    "                k=self.k,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self._metrics = self._calculate_metrics()\n",
    "\n",
    "    def _calculate_metrics(self):\n",
    "        \"\"\"Calculate ranking metrics.\"\"\"\n",
    "        self._items_for_user_pred = self.rating_pred\n",
    "\n",
    "        self._items_for_user_true = (\n",
    "            self.rating_true.groupBy(self.col_user)\n",
    "            .agg(expr(\"collect_list(\" + self.col_item + \") as ground_truth\"))\n",
    "            .select(self.col_user, \"ground_truth\")\n",
    "        )\n",
    "\n",
    "        self._items_for_user_all = self._items_for_user_pred.join(\n",
    "            self._items_for_user_true, on=self.col_user\n",
    "        ).drop(self.col_user)\n",
    "\n",
    "        return RankingMetrics(self._items_for_user_all.rdd)\n",
    "\n",
    "    def precision_at_k(self):\n",
    "        \"\"\"Get precision@k.\n",
    "\n",
    "        Note:\n",
    "            More details can be found\n",
    "            `on the precisionAt PySpark documentation <http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.precisionAt>`_.\n",
    "\n",
    "        Return:\n",
    "            float: precision at k (min=0, max=1)\n",
    "        \"\"\"\n",
    "        return self._metrics.precisionAt(self.k)\n",
    "\n",
    "    def recall_at_k(self):\n",
    "        \"\"\"Get recall@K.\n",
    "\n",
    "        Note:\n",
    "            More details can be found\n",
    "            `on the recallAt PySpark documentation <http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.recallAt>`_.\n",
    "\n",
    "        Return:\n",
    "            float: recall at k (min=0, max=1).\n",
    "        \"\"\"\n",
    "        return self._metrics.recallAt(self.k)\n",
    "\n",
    "    def ndcg_at_k(self):\n",
    "        \"\"\"Get Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "        Note:\n",
    "            More details can be found\n",
    "            `on the ndcgAt PySpark documentation <http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.ndcgAt>`_.\n",
    "\n",
    "        Return:\n",
    "            float: nDCG at k (min=0, max=1).\n",
    "        \"\"\"\n",
    "        return self._metrics.ndcgAt(self.k)\n",
    "\n",
    "    def map(self):\n",
    "        \"\"\"Get mean average precision.\n",
    "\n",
    "        Return:\n",
    "            float: MAP (min=0, max=1).\n",
    "        \"\"\"\n",
    "        return self._metrics.meanAveragePrecision\n",
    "\n",
    "    def map_at_k(self):\n",
    "        \"\"\"Get mean average precision at k.\n",
    "\n",
    "        Note:\n",
    "            More details `on the meanAveragePrecision PySpark documentation <http://spark.apache.org/docs/3.0.0/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.RankingMetrics.meanAveragePrecision>`_.\n",
    "\n",
    "        Return:\n",
    "            float: MAP at k (min=0, max=1).\n",
    "        \"\"\"\n",
    "        return self._metrics.meanAveragePrecisionAt(self.k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_top_k_items(\n",
    "    dataframe,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    k=DEFAULT_K,\n",
    "):\n",
    "    \"\"\"Get the input customer-item-rating tuple in the format of Spark\n",
    "    DataFrame, output a Spark DataFrame in the dense format of top k items\n",
    "    for each user.\n",
    "\n",
    "    Note:\n",
    "        if it is implicit rating, just append a column of constants to be ratings.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pyspark.sql.DataFrame): DataFrame of rating data (in the format of\n",
    "        customerID-itemID-rating tuple).\n",
    "        col_user (str): column name for user.\n",
    "        col_item (str): column name for item.\n",
    "        col_rating (str): column name for rating.\n",
    "        col_prediction (str): column name for prediction.\n",
    "        k (int): number of items for each user.\n",
    "\n",
    "    Return:\n",
    "        pyspark.sql.DataFrame: DataFrame of top k items for each user.\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(col_user).orderBy(col(col_rating).desc())\n",
    "\n",
    "    # this does not work for rating of the same value.\n",
    "    items_for_user = (\n",
    "        dataframe.select(\n",
    "            col_user, col_item, col_rating, row_number().over(window_spec).alias(\"rank\")\n",
    "        )\n",
    "        .where(col(\"rank\") <= k)\n",
    "        .groupby(col_user)\n",
    "        .agg(F.collect_list(col_item).alias(col_prediction))\n",
    "    )\n",
    "\n",
    "    return items_for_user\n",
    "\n",
    "\n",
    "def _get_relevant_items_by_threshold(\n",
    "    dataframe,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    threshold=DEFAULT_THRESHOLD,\n",
    "):\n",
    "    \"\"\"Get relevant items for each customer in the input rating data.\n",
    "\n",
    "    Relevant items are defined as those having ratings above certain threshold.\n",
    "    The threshold is defined as a statistical measure of the ratings for a\n",
    "    user, e.g., median.\n",
    "\n",
    "    Args:\n",
    "        dataframe: Spark DataFrame of customerID-itemID-rating tuples.\n",
    "        col_user (str): column name for user.\n",
    "        col_item (str): column name for item.\n",
    "        col_rating (str): column name for rating.\n",
    "        col_prediction (str): column name for prediction.\n",
    "        threshold (float): threshold for determining the relevant recommended items.\n",
    "            This is used for the case that predicted ratings follow a known\n",
    "            distribution.\n",
    "\n",
    "    Return:\n",
    "        pyspark.sql.DataFrame: DataFrame of customerID-itemID-rating tuples with only relevant\n",
    "        items.\n",
    "    \"\"\"\n",
    "    items_for_user = (\n",
    "        dataframe.orderBy(col_rating, ascending=False)\n",
    "        .where(col_rating + \" >= \" + str(threshold))\n",
    "        .select(col_user, col_item, col_rating)\n",
    "        .withColumn(\n",
    "            col_prediction, F.collect_list(col_item).over(Window.partitionBy(col_user))\n",
    "        )\n",
    "        .select(col_user, col_prediction)\n",
    "        .dropDuplicates()\n",
    "    )\n",
    "\n",
    "    return items_for_user\n",
    "\n",
    "\n",
    "def _get_relevant_items_by_timestamp(\n",
    "    dataframe,\n",
    "    col_user=DEFAULT_USER_COL,\n",
    "    col_item=DEFAULT_ITEM_COL,\n",
    "    col_rating=DEFAULT_RATING_COL,\n",
    "    col_timestamp=DEFAULT_TIMESTAMP_COL,\n",
    "    col_prediction=DEFAULT_PREDICTION_COL,\n",
    "    k=DEFAULT_K,\n",
    "):\n",
    "    \"\"\"Get relevant items for each customer defined by timestamp.\n",
    "\n",
    "    Relevant items are defined as k items that appear mostly recently\n",
    "    according to timestamps.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pyspark.sql.DataFrame): A Spark DataFrame of customerID-itemID-rating-timeStamp\n",
    "            tuples.\n",
    "        col_user (str): column name for user.\n",
    "        col_item (str): column name for item.\n",
    "        col_rating (str): column name for rating.\n",
    "        col_timestamp (str): column name for timestamp.\n",
    "        col_prediction (str): column name for prediction.\n",
    "        k: number of relevant items to be filtered by the function.\n",
    "\n",
    "    Return:\n",
    "        pyspark.sql.DataFrame: DataFrame of customerID-itemID-rating tuples with only relevant items.\n",
    "    \"\"\"\n",
    "    window_spec = Window.partitionBy(col_user).orderBy(col(col_timestamp).desc())\n",
    "\n",
    "    items_for_user = (\n",
    "        dataframe.select(\n",
    "            col_user, col_item, col_rating, row_number().over(window_spec).alias(\"rank\")\n",
    "        )\n",
    "        .where(col(\"rank\") <= k)\n",
    "        .withColumn(\n",
    "            col_prediction, F.collect_list(col_item).over(Window.partitionBy(col_user))\n",
    "        )\n",
    "        .select(col_user, col_prediction)\n",
    "        .dropDuplicates([col_user, col_prediction])\n",
    "    )\n",
    "\n",
    "    return items_for_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"raw_review_All_Beauty\",\n",
    "    \"raw_review_Toys_and_Games\",\n",
    "    \"raw_review_Cell_Phones_and_Accessories\",\n",
    "    \"raw_review_Industrial_and_Scientific\",\n",
    "    \"raw_review_Gift_Cards\",\n",
    "    \"raw_review_Musical_Instruments\",\n",
    "    \"raw_review_Electronics\",\n",
    "    \"raw_review_Handmade_Products\",\n",
    "    \"raw_review_Arts_Crafts_and_Sewing\",\n",
    "    \"raw_review_Baby_Products\",\n",
    "    \"raw_review_Health_and_Household\",\n",
    "    \"raw_review_Office_Products\",\n",
    "    \"raw_review_Digital_Music\",\n",
    "    \"raw_review_Grocery_and_Gourmet_Food\",\n",
    "    \"raw_review_Sports_and_Outdoors\",\n",
    "    \"raw_review_Home_and_Kitchen\",\n",
    "    \"raw_review_Subscription_Boxes\",\n",
    "    \"raw_review_Tools_and_Home_Improvement\",\n",
    "    \"raw_review_Pet_Supplies\",\n",
    "    \"raw_review_Video_Games\",\n",
    "    \"raw_review_Kindle_Store\",\n",
    "    \"raw_review_Clothing_Shoes_and_Jewelry\",\n",
    "    \"raw_review_Patio_Lawn_and_Garden\",\n",
    "    \"raw_review_Unknown\",\n",
    "    \"raw_review_Books\",\n",
    "    \"raw_review_Automotive\",\n",
    "    \"raw_review_CDs_and_Vinyl\",\n",
    "    \"raw_review_Beauty_and_Personal_Care\",\n",
    "    \"raw_review_Amazon_Fashion\",\n",
    "    \"raw_review_Magazine_Subscriptions\",\n",
    "    \"raw_review_Software\",\n",
    "    \"raw_review_Health_and_Personal_Care\",\n",
    "    \"raw_review_Appliances\",\n",
    "    \"raw_review_Movies_and_TV\"\n",
    "]\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    # StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True), \n",
    "    StructField(\"rating\", StringType(), True), \n",
    "])\n",
    "category = '5core_rating_only_All_Beauty'\n",
    "# parquet_path = f\"gs://{bucket_name}/dataset/{category}/*.parquet\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK = 10\n",
    "MAX_ITER = 5\n",
    "REG_PARAM = 0.1\n",
    "df_spark = None \n",
    "\n",
    "COL_USER = \"user_id\"\n",
    "COL_ITEM = \"parent_asin\"\n",
    "COL_RATING = \"rating\"\n",
    "COL_TIMESTAMP = \"timestamp\"\n",
    "COL_PREDICTION = \"prediction\"\n",
    "\n",
    "COL_USER_INDEX = \"user_index\"\n",
    "COL_ITEM_INDEX = \"item_index\"\n",
    "\n",
    "batch_size = 10000\n",
    "total_loaded = 0\n",
    "num_partitions = 8  \n",
    "\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Đọc dữ liệu từ Parquet\n",
    "\n",
    "# # parquet_path = f\"gs://{bucket_name}/dataset/5core_rating_only_*/*.parquet\"\n",
    "# # df_spark = spark.read.parquet(parquet_path).select(\"user_id\", \"parent_asin\", \"rating\")\n",
    "# # df_spark.write.mode(\"overwrite\").parquet(f\"gs://{bucket_name}/processed/5core_rating.parquet\")\n",
    "\n",
    "# df_spark = spark.read.parquet(f\"gs://{bucket_name}/processed/5core_rating.parquet\")\n",
    "# df_spark = df_spark.persist(StorageLevel.DISK_ONLY)\n",
    "# df_spark.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tạo indexer\n",
    "COL_USER_model = StringIndexer(inputCol=COL_USER, outputCol=COL_USER_INDEX) \\\n",
    "    .fit(df_spark.select(COL_USER).distinct())\n",
    "\n",
    "COL_ITEM_model = StringIndexer(inputCol=COL_ITEM, outputCol=COL_ITEM_INDEX) \\\n",
    "    .fit(df_spark.select(COL_ITEM).distinct())\n",
    "\n",
    "\n",
    "\n",
    "# Save model\n",
    "COL_USER_model.write().overwrite().save(f\"gs://{bucket_name}/indexer_user\")\n",
    "COL_ITEM_model.write().overwrite().save(f\"gs://{bucket_name}/indexer_item\")\n",
    "\n",
    "# COL_USER_model = StringIndexerModel.load(f\"gs://team15-storage/indexer_user\")\n",
    "# COL_ITEM_model = StringIndexerModel.load(f\"gs://team15-storage/indexer_item\")\n",
    "# Áp dụng các indexer cho DataFrame\n",
    "df_spark = COL_USER_model.transform(df_spark)\n",
    "df_spark = COL_ITEM_model.transform(df_spark)\n",
    "df_spark = df_spark.withColumn(\"rating\", df_spark[\"rating\"].cast(FloatType()))\n",
    "df_spark = df_spark.drop(\"user_id\", \"parent_asin\")\n",
    "\n",
    "df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.write.mode(\"overwrite\").parquet(f\"gs://{bucket_name}/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 14:18:23 WARN DAGScheduler: Broadcasting large task binary with size 295.7 MiB\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|rating|user_index|item_index|\n",
      "+------+----------+----------+\n",
      "|   5.0| 4067184.0| 5114046.0|\n",
      "|   5.0| 4067184.0| 5066546.0|\n",
      "|   5.0| 4067184.0| 2067665.0|\n",
      "|   5.0| 4067184.0| 5026158.0|\n",
      "|   5.0| 4067184.0| 5120623.0|\n",
      "|   4.0| 5834754.0| 2379309.0|\n",
      "|   4.0| 5834754.0| 3588803.0|\n",
      "|   5.0| 5834754.0| 1034071.0|\n",
      "|   5.0| 5834754.0| 5137683.0|\n",
      "|   5.0| 5834754.0| 4445808.0|\n",
      "|   5.0| 3135153.0| 1307667.0|\n",
      "|   5.0| 3135153.0| 3326480.0|\n",
      "|   5.0| 3135153.0| 5079509.0|\n",
      "|   5.0| 3135153.0| 2862256.0|\n",
      "|   4.0| 3135153.0| 4784473.0|\n",
      "|   5.0| 3135153.0| 4760205.0|\n",
      "|   5.0| 3135153.0| 3730944.0|\n",
      "|   5.0| 3135153.0| 3734809.0|\n",
      "|   5.0| 1043684.0| 2185788.0|\n",
      "|   5.0| 1043684.0|  677014.0|\n",
      "+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_spark = spark.read.parquet(f\"gs://{bucket_name}/dataset\")\n",
    "\n",
    "# df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_train, dfs_test = spark_random_split(df_spark, ratio=0.75, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(\n",
    "    maxIter=MAX_ITER, \n",
    "    rank=RANK,\n",
    "    regParam=REG_PARAM, \n",
    "    userCol=COL_USER_INDEX, \n",
    "    itemCol=COL_ITEM_INDEX, \n",
    "    ratingCol=COL_RATING, \n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "###\n",
    "\n",
    "model = als.fit(dfs_train)\n",
    "\n",
    "dfs_pred = model.transform(dfs_test).drop(COL_RATING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"team15-storage\"\n",
    "model_path = f\"gs://{bucket_name}/als_model\" \n",
    "model.write().overwrite().save(model_path)\n",
    "print(f\"Mô hình ALS đã được lưu vào: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alias DataFrames\n",
    "pred_alias = dfs_pred.alias(\"pred\")\n",
    "train_alias = dfs_train.alias(\"train\")\n",
    "\n",
    "# Join và dùng col(\"alias.colname\") thay vì dfs[col]\n",
    "dfs_pred_exclude_train = pred_alias.join(\n",
    "    train_alias,\n",
    "    (col(\"pred.\" + COL_USER_INDEX) == col(\"train.\" + COL_USER_INDEX)) &\n",
    "    (col(\"pred.\" + COL_ITEM_INDEX) == col(\"train.\" + COL_ITEM_INDEX)),\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Filter những cặp chưa thấy trong training\n",
    "dfs_pred_final = dfs_pred_exclude_train.filter(col(\"train.Rating\").isNull()) \\\n",
    "    .select(\n",
    "        col(\"pred.\" + COL_USER_INDEX).alias(COL_USER_INDEX),\n",
    "        col(\"pred.\" + COL_ITEM_INDEX).alias(COL_ITEM_INDEX),\n",
    "        col(\"pred.prediction\")\n",
    "    )\n",
    "\n",
    "dfs_pred_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 14:19:21 WARN DAGScheduler: Broadcasting large task binary with size 295.7 MiB\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|user_index|item_index|prediction|\n",
      "+----------+----------+----------+\n",
      "|      16.0| 1401953.0| 4.5411286|\n",
      "|      23.0| 2945575.0|  5.065992|\n",
      "|      64.0| 4817381.0|  4.502164|\n",
      "|      65.0|  915115.0|  4.400567|\n",
      "|      69.0| 3146871.0|  4.442011|\n",
      "|      74.0| 1193036.0| 2.1374714|\n",
      "|      83.0| 2317153.0| 4.5951037|\n",
      "|      83.0| 3623070.0|  4.737312|\n",
      "|      83.0| 4244158.0|  4.670016|\n",
      "|      97.0| 1308857.0|  4.599385|\n",
      "|     105.0| 4919669.0| 3.1340566|\n",
      "|     145.0| 4212876.0|  4.211695|\n",
      "|     153.0| 1553354.0|  3.707421|\n",
      "|     168.0| 1331337.0|  4.152615|\n",
      "|     168.0| 2799882.0|  4.357819|\n",
      "|     168.0| 3038503.0| 4.4673553|\n",
      "|     182.0| 1092397.0| 2.7466314|\n",
      "|     194.0| 2087264.0| 4.2487354|\n",
      "|     223.0| 3776419.0|  4.800173|\n",
      "|     256.0| 4165538.0| 4.7921405|\n",
      "+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dfs_pred_final.write.mode(\"overwrite\").parquet(f\"gs://{bucket_name}/predict\")\n",
    "dfs_pred_final = spark.read.parquet(f\"gs://{bucket_name}/predict\")\n",
    "\n",
    "dfs_pred_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 15:19:53 WARN DAGScheduler: Broadcasting large task binary with size 295.7 MiB\n",
      "25/04/21 15:20:05 WARN DAGScheduler: Broadcasting large task binary with size 295.7 MiB\n",
      "25/04/21 15:39:28 WARN DAGScheduler: Broadcasting large task binary with size 295.7 MiB\n",
      "25/04/21 15:40:54 WARN DAGScheduler: Broadcasting large task binary with size 295.7 MiB\n",
      "25/04/21 16:05:56 WARN DAGScheduler: Broadcasting large task binary with size 295.7 MiB\n",
      "25/04/21 16:06:56 WARN DAGScheduler: Broadcasting large task binary with size 591.5 MiB\n",
      "25/04/21 16:07:21 WARN DAGScheduler: Broadcasting large task binary with size 591.5 MiB\n",
      "25/04/21 16:08:33 WARN DAGScheduler: Broadcasting large task binary with size 591.5 MiB\n",
      "25/04/21 16:09:47 WARN DAGScheduler: Broadcasting large task binary with size 591.5 MiB\n",
      "25/04/21 16:10:59 WARN DAGScheduler: Broadcasting large task binary with size 591.5 MiB\n",
      "[Stage 81:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@k = 0.6091560059645985\n",
      "Recall@k = 0.827802918653073\n",
      "NDCG@k = 0.9811180097721983\n",
      "Mean average precision = 0.974869135836306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluations = SparkRankingEvaluation(\n",
    "    dfs_test, \n",
    "    dfs_pred_final,\n",
    "    col_user=COL_USER_INDEX,\n",
    "    col_item=COL_ITEM_INDEX,\n",
    "    col_rating=COL_RATING,\n",
    "    col_prediction=COL_PREDICTION,\n",
    "    k=K\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Precision@k = {}\".format(evaluations.precision_at_k()),\n",
    "    \"Recall@k = {}\".format(evaluations.recall_at_k()),\n",
    "    \"NDCG@k = {}\".format(evaluations.ndcg_at_k()),\n",
    "    \"Mean average precision = {}\".format(evaluations.map_at_k()),\n",
    "    sep=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\n",
    "    2715975.0,\n",
    "    541078.0,\n",
    "    1717487.0,\n",
    "    4142059.0,\n",
    "    377882.0,\n",
    "    5525177.0,\n",
    "    4429053.0,\n",
    "    1753193.0,\n",
    "    4157055.0,\n",
    "    1726869.0\n",
    "]\n",
    "\n",
    "dfs_rec_subset = model.recommendForUserSubset(users, 10)\n",
    "dfs_rec_subset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cleanup spark instance\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
