{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-storage datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import itertools\n",
    "import time\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "# ------------------------- CONFIG -------------------------\n",
    "CREDENTIAL_PATH = \"/kaggle/input/bigdata-inj/bigdataproject-456014-42479bab67be.json\"\n",
    "BUCKET_NAME = \"team15-storage\"\n",
    "BATCH_SIZE = 50000  # Increased for better performance\n",
    "NUM_PARTITIONS = 24  # Increased to parallelize better\n",
    "\n",
    "# Categories grouped by size\n",
    "small_categories = [\n",
    "    \"raw_review_Magazine_Subscriptions\",\n",
    "    \"raw_review_Gift_Cards\",\n",
    "    \"raw_review_All_Beauty\",\n",
    "    \"raw_review_Software\",\n",
    "    \"raw_review_Musical_Instruments\",\n",
    "    \"raw_review_Industrial_and_Scientific\"\n",
    "]\n",
    "\n",
    "medium_categories = [\n",
    "    \"raw_review_Baby_Products\",\n",
    "    \"raw_review_Office_Products\",\n",
    "    \"raw_review_CDs_and_Vinyl\",\n",
    "    \"raw_review_Arts_Crafts_and_Sewing\",\n",
    "    \"raw_review_Cell_Phones_and_Accessories\",\n",
    "    \"raw_review_Pet_Supplies\",\n",
    "    \"raw_review_Grocery_and_Gourmet_Food\",\n",
    "    \"raw_review_Patio_Lawn_and_Garden\",\n",
    "    \"raw_review_Toys_and_Games\",\n",
    "    \"raw_review_Health_and_Household\",\n",
    "    \"raw_review_Movies_and_TV\",\n",
    "    \"raw_review_Beauty_and_Personal_Care\"\n",
    "]\n",
    "\n",
    "large_categories = [\n",
    "    \"raw_review_Automotive\",\n",
    "    \"raw_review_Tools_and_Home_Improvement\",\n",
    "    \"raw_review_Electronics\",\n",
    "    \"raw_review_Books\",\n",
    "    \"raw_review_Clothing_Shoes_and_Jewelry\",\n",
    "    \"raw_review_Home_and_Kitchen\"\n",
    "]\n",
    "\n",
    "raw_review_categories = small_categories + medium_categories + large_categories\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", StringType(), True),\n",
    "    StructField(\"rating_number\", StringType(), True),\n",
    "])\n",
    "\n",
    "spinner = itertools.cycle([\"-\", \"\\\\\", \"|\", \"/\"])\n",
    "\n",
    "# --------------------- INIT SPARK & GCS --------------------\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = CREDENTIAL_PATH\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AmazonReviews\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET_NAME)\n",
    "\n",
    "print(\"Spark Master:\", spark.sparkContext.master)\n",
    "\n",
    "# Track completed datasets\n",
    "completed_file = \"completed_categories.txt\"\n",
    "if os.path.exists(completed_file):\n",
    "    with open(completed_file) as f:\n",
    "        completed = set(line.strip() for line in f)\n",
    "else:\n",
    "    completed = set()\n",
    "\n",
    "# ----------------------- MAIN PIPELINE ----------------------\n",
    "for raw_cat in raw_review_categories:\n",
    "    cat_name = raw_cat.replace(\"raw_review_\", \"\")\n",
    "    dataset_name = f\"raw_meta_{cat_name}\"\n",
    "\n",
    "    if dataset_name in completed:\n",
    "        print(f\"⏭ Skipping {dataset_name} (already done)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n Loading dataset: {dataset_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", dataset_name, streaming=True, trust_remote_code=True)\n",
    "        data_iter = iter(dataset[\"full\"])\n",
    "\n",
    "        total_loaded = 0\n",
    "        batch_count = 0\n",
    "\n",
    "        while True:\n",
    "            batch = list(islice(data_iter, BATCH_SIZE))\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "            rdd = spark.sparkContext.parallelize(batch, numSlices=NUM_PARTITIONS)\n",
    "            df_batch = spark.createDataFrame(rdd, schema=schema)\n",
    "\n",
    "            temp_parquet_path = f\"temp_{cat_name}_batch_{batch_count}.parquet\"\n",
    "            df_batch.write.mode(\"overwrite\").parquet(temp_parquet_path)\n",
    "\n",
    "            part_files = glob.glob(os.path.join(temp_parquet_path, \"part-*.parquet\"))\n",
    "            for part_file in part_files:\n",
    "                try:\n",
    "                    blob = bucket.blob(f\"item/{dataset_name}/{os.path.basename(part_file)}\")\n",
    "                    blob.upload_from_filename(part_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error uploading {part_file}: {e}\")\n",
    "\n",
    "            shutil.rmtree(temp_parquet_path)\n",
    "\n",
    "            total_loaded += len(batch)\n",
    "            sys.stdout.write(f\"\\r{next(spinner)} Loading {total_loaded:,} rows from {dataset_name}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            batch_count += 1\n",
    "\n",
    "        duration = (time.time() - start_time) / 60\n",
    "        print(f\"\\n✅ Completed {dataset_name} in {duration:.2f} min: {total_loaded:,} rows uploaded.\")\n",
    "\n",
    "        with open(completed_file, \"a\") as f:\n",
    "            f.write(dataset_name + \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to load {dataset_name}: {e}\")\n",
    "\n",
    "print(\"\\nAll categories processed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7153939,
     "sourceId": 11423073,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
